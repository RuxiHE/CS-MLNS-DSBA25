{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cck/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, TransformerConv, GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import csv\n",
    "import os\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "node_info_path = os.path.join(base_path, \"node_information.csv\")\n",
    "train_path = os.path.join(base_path, \"train.txt\")\n",
    "test_path = os.path.join(base_path, \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = {}\n",
    "with open(node_info_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split(',') \n",
    "        node_id = int(values[0])\n",
    "        features = np.array([float(x) for x in values[1:]])\n",
    "        node_features[node_id] = features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum node ID to determine feature matrix size\n",
    "max_node_id = max(node_features.keys())\n",
    "feature_size = len(next(iter(node_features.values())))\n",
    "\n",
    "# Create feature matrix\n",
    "X = np.zeros((max_node_id + 1, feature_size))\n",
    "for node_id, features in node_features.items():\n",
    "    X[node_id] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges = []\n",
    "train_labels = []\n",
    "with open(train_path , \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        node1 = int(values[0])\n",
    "        node2 = int(values[1])\n",
    "        label = int(values[2])\n",
    "        train_edges.append((node1, node2))\n",
    "        train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edges = []\n",
    "with open(test_path , \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        node1 = int(values[0])\n",
    "        node2 = int(values[1])\n",
    "        test_edges.append((node1, node2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYG Ready Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "x = torch.FloatTensor(X)\n",
    "\n",
    "# Create positive and negative edge indices\n",
    "pos_edge_index = []\n",
    "neg_edge_index = []\n",
    "\n",
    "for edge, label in zip(train_edges, train_labels):\n",
    "    if label == 1:\n",
    "        pos_edge_index.append(edge)\n",
    "    else:\n",
    "        neg_edge_index.append(edge)\n",
    "\n",
    "pos_edge_index = torch.tensor(pos_edge_index, dtype=torch.long).t()\n",
    "neg_edge_index = torch.tensor(neg_edge_index, dtype=torch.long).t()\n",
    "\n",
    "# Create a PyG Data object\n",
    "data = Data(x=x, num_nodes=max_node_id+1)\n",
    "\n",
    "# Split positive edges for training and validation\n",
    "data.train_pos_edge_index = pos_edge_index\n",
    "data.train_neg_edge_index = neg_edge_index\n",
    "\n",
    "# Create a combined edge index for message passing\n",
    "edge_index = torch.cat([pos_edge_index, pos_edge_index.flip(0)], dim=1)  # Make it undirected\n",
    "data.edge_index = edge_index\n",
    "\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN / SAGE / GAT / Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, model_type='GCN'):\n",
    "        super(GNNLinkPredictor, self).__init__()\n",
    "\n",
    "        # Initialize the convolutional layers\n",
    "        if model_type == 'GCN':\n",
    "            self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "            self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "        elif model_type == 'SAGE':\n",
    "            self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "        elif model_type == 'GAT':\n",
    "            self.conv1 = GATConv(in_channels, hidden_channels)\n",
    "            self.conv2 = GATConv(hidden_channels, out_channels)\n",
    "\n",
    "        elif model_type == 'GAT2':\n",
    "            self.conv1 = GATv2Conv(in_channels, hidden_channels)\n",
    "            self.conv2 = GATv2Conv(hidden_channels, out_channels)\n",
    "\n",
    "        elif model_type == 'Transformer':\n",
    "            self.conv1 = TransformerConv(in_channels, hidden_channels)\n",
    "            self.conv2 = TransformerConv(hidden_channels, out_channels)\n",
    "\n",
    "        # Initialize the link prediction layer\n",
    "        self.link_predict = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    # encode the input graph data to produce a latent node representation.\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    # decode the latent node representation to predict whether an edge exists between two nodes.\n",
    "    def decode(self, z, edge_index):\n",
    "        # get the source and target node indices\n",
    "        src, tgt = edge_index\n",
    "        # combine the node embeddings of the source and target nodes\n",
    "        return self.link_predict(torch.cat([z[src], z[tgt]], dim=1))\n",
    "\n",
    "    def forward(self, x, edge_index, predict_edges=None):\n",
    "        # encode the input graph data\n",
    "        z = self.encode(x, edge_index)\n",
    "        \n",
    "        # If we're given specific edges to predict, only predict those\n",
    "        if predict_edges is not None:\n",
    "            return self.decode(z, predict_edges)\n",
    "        \n",
    "        # Otherwise, predict all edges\n",
    "        return self.decode(z, edge_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validation (Choose the Model (GCN/SAGE/GAT/GAT2/Transformer) Here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "split_size = int(len(train_labels) * split_ratio)\n",
    "\n",
    "# Validation Data\n",
    "val_edges = train_edges[split_size:]\n",
    "val_labels = train_labels[split_size:]\n",
    "val_edge_index = torch.tensor(val_edges, dtype=torch.long).t().to(device)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.float).to(device)\n",
    "    \n",
    "\n",
    "# Parameters\n",
    "in_channels, hidden_channels, out_channels, num_epochs = data.num_features, 128, 64, 100\n",
    "\n",
    "model_type = 'GCN' # Default: 'GCN\", Options: 'SAGE', 'GAT', 'GAT2', 'Transformer'\n",
    "model = GNNLinkPredictor(in_channels, hidden_channels, out_channels, model_type).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "best_val_auc, best_epoch, patience, counter = 0, 0, 10, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, batch_size=64):\n",
    "    model.train()\n",
    "    \n",
    "    # Create positive and negative examples\n",
    "    pos_edge_index = data.train_pos_edge_index\n",
    "    neg_edge_index = data.train_neg_edge_index\n",
    "    \n",
    "    # Shuffle edges\n",
    "    pos_rand = torch.randperm(pos_edge_index.size(1))\n",
    "    neg_rand = torch.randperm(neg_edge_index.size(1))\n",
    "    \n",
    "    # Split edges into batches\n",
    "    pos_edge_index = pos_edge_index[:, pos_rand]\n",
    "    neg_edge_index = neg_edge_index[:, neg_rand]\n",
    "    \n",
    "    # Limit to the smaller set's size\n",
    "    size_min = min(pos_edge_index.size(1), neg_edge_index.size(1))\n",
    "    pos_edge_index = pos_edge_index[:, :size_min]\n",
    "    neg_edge_index = neg_edge_index[:, :size_min]\n",
    "    \n",
    "    # Process in batches to save memory\n",
    "    total_loss = 0\n",
    "    num_batches = (size_min + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        \n",
    "        # Get batch size\n",
    "        start = batch_idx * batch_size\n",
    "        end = min((batch_idx + 1) * batch_size, size_min)\n",
    "        \n",
    "        # Create positive and negative batch examples\n",
    "        pos_batch = pos_edge_index[:, start:end]\n",
    "        neg_batch = neg_edge_index[:, start:end]\n",
    "        \n",
    "        # Create target labels\n",
    "        pos_y = torch.ones(pos_batch.size(1), 1, device=pos_batch.device)\n",
    "        neg_y = torch.zeros(neg_batch.size(1), 1, device=neg_batch.device)\n",
    "        \n",
    "        # Get predictions\n",
    "        pos_pred = model(data.x, data.edge_index, pos_batch)\n",
    "        neg_pred = model(data.x, data.edge_index, neg_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            torch.cat([pos_pred, neg_pred], dim=0), # concatenate positive and negative predictions\n",
    "            torch.cat([pos_y, neg_y], dim=0) # concatenate positive and negative labels\n",
    "        )\n",
    "        \n",
    "        loss.backward() # compute gradients\n",
    "        optimizer.step() # update weights\n",
    "        total_loss += loss.item() # Accumulate loss\n",
    "    \n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, edge_index, labels):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad(): # Do not calculate gradients\n",
    "        z = model.encode(data.x, data.edge_index) \n",
    "        s = model.decode(z, edge_index)\n",
    "        pred = torch.sigmoid(s).cpu().numpy()\n",
    "        \n",
    "\n",
    "        # Calculate AUC\n",
    "        auc = roc_auc_score(labels, pred)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        pred_labels = (pred > 0.5).astype(int)\n",
    "        acc = accuracy_score(labels, pred_labels)\n",
    "        \n",
    "    return auc, acc, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN model...\n",
      "Epoch: 001, Loss: 0.6322, Val AUC: 0.8138, Val Acc: 0.7338\n",
      "Epoch: 002, Loss: 0.5381, Val AUC: 0.8424, Val Acc: 0.7533\n",
      "Epoch: 003, Loss: 0.5161, Val AUC: 0.8505, Val Acc: 0.7590\n",
      "Epoch: 004, Loss: 0.5045, Val AUC: 0.8564, Val Acc: 0.7710\n",
      "Epoch: 005, Loss: 0.5003, Val AUC: 0.8665, Val Acc: 0.7714\n",
      "Epoch: 006, Loss: 0.4883, Val AUC: 0.8688, Val Acc: 0.7786\n",
      "Epoch: 007, Loss: 0.4789, Val AUC: 0.8754, Val Acc: 0.7833\n",
      "Epoch: 008, Loss: 0.4616, Val AUC: 0.8848, Val Acc: 0.7933\n",
      "Epoch: 009, Loss: 0.4460, Val AUC: 0.9008, Val Acc: 0.8224\n",
      "Epoch: 010, Loss: 0.4220, Val AUC: 0.9236, Val Acc: 0.8638\n",
      "Epoch: 011, Loss: 0.3886, Val AUC: 0.9368, Val Acc: 0.8795\n",
      "Epoch: 012, Loss: 0.3632, Val AUC: 0.9425, Val Acc: 0.8981\n",
      "Epoch: 013, Loss: 0.3579, Val AUC: 0.9435, Val Acc: 0.8919\n",
      "Epoch: 014, Loss: 0.3414, Val AUC: 0.9463, Val Acc: 0.8857\n",
      "Epoch: 015, Loss: 0.3381, Val AUC: 0.9501, Val Acc: 0.9048\n",
      "Epoch: 016, Loss: 0.3319, Val AUC: 0.9528, Val Acc: 0.9043\n",
      "Epoch: 017, Loss: 0.3241, Val AUC: 0.9549, Val Acc: 0.9095\n",
      "Epoch: 018, Loss: 0.3239, Val AUC: 0.9546, Val Acc: 0.9057\n",
      "Epoch: 019, Loss: 0.3194, Val AUC: 0.9576, Val Acc: 0.9152\n",
      "Epoch: 020, Loss: 0.3125, Val AUC: 0.9580, Val Acc: 0.9205\n",
      "Epoch: 021, Loss: 0.3128, Val AUC: 0.9584, Val Acc: 0.9038\n",
      "Epoch: 022, Loss: 0.3095, Val AUC: 0.9598, Val Acc: 0.9129\n",
      "Epoch: 023, Loss: 0.3009, Val AUC: 0.9612, Val Acc: 0.9195\n",
      "Epoch: 024, Loss: 0.2964, Val AUC: 0.9628, Val Acc: 0.9205\n",
      "Epoch: 025, Loss: 0.2900, Val AUC: 0.9637, Val Acc: 0.9229\n",
      "Epoch: 026, Loss: 0.2881, Val AUC: 0.9644, Val Acc: 0.9271\n",
      "Epoch: 027, Loss: 0.2856, Val AUC: 0.9654, Val Acc: 0.9300\n",
      "Epoch: 028, Loss: 0.2827, Val AUC: 0.9667, Val Acc: 0.9286\n",
      "Epoch: 029, Loss: 0.2811, Val AUC: 0.9670, Val Acc: 0.9319\n",
      "Epoch: 030, Loss: 0.2789, Val AUC: 0.9678, Val Acc: 0.9281\n",
      "Epoch: 031, Loss: 0.2776, Val AUC: 0.9683, Val Acc: 0.9305\n",
      "Epoch: 032, Loss: 0.2790, Val AUC: 0.9702, Val Acc: 0.9300\n",
      "Epoch: 033, Loss: 0.2726, Val AUC: 0.9703, Val Acc: 0.9295\n",
      "Epoch: 034, Loss: 0.2603, Val AUC: 0.9696, Val Acc: 0.9290\n",
      "Epoch: 035, Loss: 0.2597, Val AUC: 0.9718, Val Acc: 0.9362\n",
      "Epoch: 036, Loss: 0.2666, Val AUC: 0.9745, Val Acc: 0.9400\n",
      "Epoch: 037, Loss: 0.2602, Val AUC: 0.9741, Val Acc: 0.9376\n",
      "Epoch: 038, Loss: 0.2584, Val AUC: 0.9736, Val Acc: 0.9410\n",
      "Epoch: 039, Loss: 0.2564, Val AUC: 0.9741, Val Acc: 0.9452\n",
      "Epoch: 040, Loss: 0.2567, Val AUC: 0.9734, Val Acc: 0.9381\n",
      "Epoch: 041, Loss: 0.2508, Val AUC: 0.9747, Val Acc: 0.9390\n",
      "Epoch: 042, Loss: 0.2457, Val AUC: 0.9744, Val Acc: 0.9371\n",
      "Epoch: 043, Loss: 0.2544, Val AUC: 0.9750, Val Acc: 0.9471\n",
      "Epoch: 044, Loss: 0.2465, Val AUC: 0.9749, Val Acc: 0.9419\n",
      "Epoch: 045, Loss: 0.2441, Val AUC: 0.9770, Val Acc: 0.9490\n",
      "Epoch: 046, Loss: 0.2414, Val AUC: 0.9769, Val Acc: 0.9452\n",
      "Epoch: 047, Loss: 0.2387, Val AUC: 0.9772, Val Acc: 0.9490\n",
      "Epoch: 048, Loss: 0.2421, Val AUC: 0.9768, Val Acc: 0.9462\n",
      "Epoch: 049, Loss: 0.2406, Val AUC: 0.9777, Val Acc: 0.9495\n",
      "Epoch: 050, Loss: 0.2329, Val AUC: 0.9788, Val Acc: 0.9462\n",
      "Epoch: 051, Loss: 0.2292, Val AUC: 0.9790, Val Acc: 0.9433\n",
      "Epoch: 052, Loss: 0.2281, Val AUC: 0.9805, Val Acc: 0.9529\n",
      "Epoch: 053, Loss: 0.2303, Val AUC: 0.9807, Val Acc: 0.9424\n",
      "Epoch: 054, Loss: 0.2276, Val AUC: 0.9813, Val Acc: 0.9500\n",
      "Epoch: 055, Loss: 0.2254, Val AUC: 0.9803, Val Acc: 0.9490\n",
      "Epoch: 056, Loss: 0.2274, Val AUC: 0.9808, Val Acc: 0.9529\n",
      "Epoch: 057, Loss: 0.2212, Val AUC: 0.9816, Val Acc: 0.9533\n",
      "Epoch: 058, Loss: 0.2162, Val AUC: 0.9802, Val Acc: 0.9519\n",
      "Epoch: 059, Loss: 0.2172, Val AUC: 0.9826, Val Acc: 0.9581\n",
      "Epoch: 060, Loss: 0.2150, Val AUC: 0.9830, Val Acc: 0.9452\n",
      "Epoch: 061, Loss: 0.2187, Val AUC: 0.9828, Val Acc: 0.9538\n",
      "Epoch: 062, Loss: 0.2147, Val AUC: 0.9840, Val Acc: 0.9581\n",
      "Epoch: 063, Loss: 0.2128, Val AUC: 0.9830, Val Acc: 0.9457\n",
      "Epoch: 064, Loss: 0.2132, Val AUC: 0.9835, Val Acc: 0.9557\n",
      "Epoch: 065, Loss: 0.2100, Val AUC: 0.9849, Val Acc: 0.9614\n",
      "Epoch: 066, Loss: 0.2020, Val AUC: 0.9852, Val Acc: 0.9624\n",
      "Epoch: 067, Loss: 0.2029, Val AUC: 0.9846, Val Acc: 0.9605\n",
      "Epoch: 068, Loss: 0.1957, Val AUC: 0.9855, Val Acc: 0.9567\n",
      "Epoch: 069, Loss: 0.2037, Val AUC: 0.9860, Val Acc: 0.9629\n",
      "Epoch: 070, Loss: 0.1952, Val AUC: 0.9859, Val Acc: 0.9595\n",
      "Epoch: 071, Loss: 0.1948, Val AUC: 0.9867, Val Acc: 0.9586\n",
      "Epoch: 072, Loss: 0.1949, Val AUC: 0.9872, Val Acc: 0.9610\n",
      "Epoch: 073, Loss: 0.1950, Val AUC: 0.9864, Val Acc: 0.9667\n",
      "Epoch: 074, Loss: 0.1936, Val AUC: 0.9866, Val Acc: 0.9643\n",
      "Epoch: 075, Loss: 0.1977, Val AUC: 0.9867, Val Acc: 0.9624\n",
      "Epoch: 076, Loss: 0.1934, Val AUC: 0.9873, Val Acc: 0.9671\n",
      "Epoch: 077, Loss: 0.1878, Val AUC: 0.9883, Val Acc: 0.9681\n",
      "Epoch: 078, Loss: 0.1872, Val AUC: 0.9889, Val Acc: 0.9652\n",
      "Epoch: 079, Loss: 0.1879, Val AUC: 0.9889, Val Acc: 0.9676\n",
      "Epoch: 080, Loss: 0.1817, Val AUC: 0.9890, Val Acc: 0.9652\n",
      "Epoch: 081, Loss: 0.1864, Val AUC: 0.9899, Val Acc: 0.9667\n",
      "Epoch: 082, Loss: 0.1816, Val AUC: 0.9905, Val Acc: 0.9681\n",
      "Epoch: 083, Loss: 0.1743, Val AUC: 0.9922, Val Acc: 0.9719\n",
      "Epoch: 084, Loss: 0.1742, Val AUC: 0.9921, Val Acc: 0.9643\n",
      "Epoch: 085, Loss: 0.1760, Val AUC: 0.9927, Val Acc: 0.9724\n",
      "Epoch: 086, Loss: 0.1781, Val AUC: 0.9928, Val Acc: 0.9729\n",
      "Epoch: 087, Loss: 0.1785, Val AUC: 0.9932, Val Acc: 0.9719\n",
      "Epoch: 088, Loss: 0.1695, Val AUC: 0.9931, Val Acc: 0.9748\n",
      "Epoch: 089, Loss: 0.1695, Val AUC: 0.9935, Val Acc: 0.9657\n",
      "Epoch: 090, Loss: 0.1678, Val AUC: 0.9944, Val Acc: 0.9714\n",
      "Epoch: 091, Loss: 0.1752, Val AUC: 0.9940, Val Acc: 0.9748\n",
      "Epoch: 092, Loss: 0.1663, Val AUC: 0.9945, Val Acc: 0.9786\n",
      "Epoch: 093, Loss: 0.1626, Val AUC: 0.9940, Val Acc: 0.9729\n",
      "Epoch: 094, Loss: 0.1608, Val AUC: 0.9953, Val Acc: 0.9810\n",
      "Epoch: 095, Loss: 0.1556, Val AUC: 0.9947, Val Acc: 0.9814\n",
      "Epoch: 096, Loss: 0.1563, Val AUC: 0.9951, Val Acc: 0.9810\n",
      "Epoch: 097, Loss: 0.1544, Val AUC: 0.9950, Val Acc: 0.9767\n",
      "Epoch: 098, Loss: 0.1603, Val AUC: 0.9952, Val Acc: 0.9810\n",
      "Epoch: 099, Loss: 0.1530, Val AUC: 0.9955, Val Acc: 0.9800\n",
      "Epoch: 100, Loss: 0.1548, Val AUC: 0.9958, Val Acc: 0.9862\n",
      "Best model at epoch 100 with validation AUC: 0.9958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/t1f6lt2x0xz0nsqj_rj6382m0000gn/T/ipykernel_63870/4045853769.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd = os.getcwd()\n",
    "model_path = os.path.join(wd, \"best_gnn_model.pt\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training {model_type} model...\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(model, data, optimizer)\n",
    "    \n",
    "    # Validation\n",
    "    val_auc, val_acc, _ = evaluate(model, data, val_edge_index, val_labels.cpu().numpy())\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AUC: {val_auc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_epoch = epoch\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(f\"Best model at epoch {best_epoch} with validation AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, data, device, val_edge_index, val_labels):\n",
    "    # Model parameters\n",
    "    hidden_channels = trial.suggest_int('hidden_dim', 32, 256)\n",
    "    out_channels = trial.suggest_int('out_dim', 16, 128)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    in_channels = data.num_features\n",
    "\n",
    "    # Model parameters\n",
    "    model = GNNLinkPredictor(in_channels, hidden_channels, out_channels, model_type=\"GCN\").to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_auc = 0\n",
    "    # best_epoch = 0\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "\n",
    "    num_epochs = 100\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss = train(model, data, optimizer)\n",
    "\n",
    "        # Validation\n",
    "        val_auc, val_acc, _ = evaluate(model, data, val_edge_index, val_labels.cpu().numpy())\n",
    "\n",
    "        # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AUC: {val_auc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_epoch = epoch\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    return best_val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, data, device, val_edge_index, val_labels), n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"Best params: {best_params}\")\n",
    "\n",
    "model = GNNLinkPredictor(data.num_features, best_params['hidden_dim'], best_params['out_dim'], model_type=\"GCN\").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'], weight_decay=5e-4)\n",
    "\n",
    "model_path = os.path.join(base_path, \"best_gnn_model.pt\")\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(model, data, optimizer)\n",
    "\n",
    "    # Validation\n",
    "    val_auc, val_acc, _ = evaluate(model, data, val_edge_index, val_labels.cpu().numpy())\n",
    "\n",
    "    # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AUC: {val_auc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_epoch = epoch\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(f\"Best model at epoch {best_epoch} with validation AUC: {best_val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, data, test_edges):\n",
    "    model.eval()\n",
    "\n",
    "    wd = os.getcwd()\n",
    "    output_file = os.path.join(wd, \"gnn.csv\")\n",
    "\n",
    "    # Convert test edges to tensor\n",
    "    test_edge_index = torch.tensor(test_edges, dtype=torch.long).t()\n",
    "    \n",
    "    with torch.no_grad(): # Do not calculate gradients\n",
    "        # Make predictions\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "        s = model.decode(z, test_edge_index)\n",
    "        pred_probs = torch.sigmoid(s).cpu().numpy().flatten()\n",
    "        \n",
    "        # Get binary predictions\n",
    "        predictions = (pred_probs > 0.5).astype(int)\n",
    "        \n",
    "        # Output predictions\n",
    "        with open(output_file, \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"ID\", \"Predicted\"])\n",
    "            for i, pred in enumerate(predictions):\n",
    "                writer.writerow([i, int(pred)])\n",
    "        \n",
    "        print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /Users/cck/Desktop/OneDrive_France/DSBA/T2/Machine Learning in Network Science/Kaggle/gnn.csv\n"
     ]
    }
   ],
   "source": [
    "predict_test(model, data, test_edges) # predict and output the prediction result to a csv file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
