{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter (Yes or No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_HYPERPARAMETER_TUNING = True # Default: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "node_info_path = os.path.join(base_path, \"node_information.csv\")\n",
    "train_path = os.path.join(base_path, \"train.txt\")\n",
    "test_path = os.path.join(base_path, \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = {}\n",
    "with open(node_info_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split(',')\n",
    "        node_id = int(values[0])\n",
    "        features = np.array([float(x) for x in values[1:]])\n",
    "        node_features[node_id] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum node ID to determine feature matrix size\n",
    "max_node_id = max(node_features.keys())\n",
    "feature_size = len(next(iter(node_features.values())))\n",
    "\n",
    "# Create feature matrix\n",
    "X = np.zeros((max_node_id + 1, feature_size))\n",
    "for node_id, features in node_features.items():\n",
    "    X[node_id] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges = []\n",
    "train_labels = []\n",
    "with open(train_path , \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        node1 = int(values[0])\n",
    "        node2 = int(values[1])\n",
    "        label = int(values[2])\n",
    "        train_edges.append((node1, node2))\n",
    "        train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edges = []\n",
    "with open(test_path , \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        node1 = int(values[0])\n",
    "        node2 = int(values[1])\n",
    "        test_edges.append((node1, node2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize node features\n",
    "stdscaler = StandardScaler()\n",
    "node_features = stdscaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(edges, max_node_id):\n",
    "    G = nx.Graph() # undirected graph\n",
    "    G.add_nodes_from(range(max_node_id + 1)) # Add all nodes from 0 to max_node_id (including isolated nodes)\n",
    "    for edge in edges: # Add edges\n",
    "        u, v = edge\n",
    "        G.add_edge(u, v)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training graph using only positive edges\n",
    "pos_edges = [edge for edge, label in zip(train_edges, train_labels) if label == 1]\n",
    "train_graph = build_graph(pos_edges, max_node_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(edges, graph, node_features, use_topology=True):\n",
    "    features_list = []\n",
    "\n",
    "    for u, v in tqdm(edges, desc=\"Extracting features\"):\n",
    "        # Node features computation\n",
    "        feat_u = node_features[u]\n",
    "        feat_v = node_features[v]\n",
    "        \n",
    "        # Features Operations\n",
    "        diff = np.abs(feat_u - feat_v)  # absolute difference: emphasize the difference between nodes\n",
    "        mult = feat_u * feat_v          # element-wise product: emphasize common important features\n",
    "        \n",
    "        # dimensionality reduction\n",
    "        combined_features = np.concatenate([\n",
    "            diff,   # absolute difference\n",
    "            mult    # element-wise product\n",
    "        ])\n",
    "        \n",
    "        # topology feature（Graph structure feature）(designed to capture the structural relationships between nodes in the graph)\n",
    "        if use_topology and u in graph and v in graph:\n",
    "            # Common neighbors: Nodes with more common neighbors are more likely to be connected\n",
    "            cn = len(list(nx.common_neighbors(graph, u, v)))\n",
    "            \n",
    "\n",
    "            # Jaccard coefficient:  Measures the similarity of the neighbors of the two nodes\n",
    "            neighbors_u = set(graph.neighbors(u))\n",
    "            neighbors_v = set(graph.neighbors(v))\n",
    "            union_size = len(neighbors_u.union(neighbors_v))\n",
    "            jaccard = cn / union_size if union_size > 0 else 0\n",
    "            \n",
    "            # Preferential attachment: Nodes with higher degrees are more likely to be connected\n",
    "            pa = graph.degree(u) * graph.degree(v)\n",
    "            \n",
    "            # Adamic-Adar index: Gives more weight to low-degree common neighbors when computing similarity\n",
    "            aa = sum(1 / np.log(graph.degree(w)) for w in nx.common_neighbors(graph, u, v) if graph.degree(w) > 1)\n",
    "            \n",
    "            # Resource allocation index: Assumes resource is allocated based on node degrees\n",
    "            ra = sum(1 / graph.degree(w) for w in nx.common_neighbors(graph, u, v) if graph.degree(w) > 0)\n",
    "            \n",
    "            # Node degree: Captures the \"activeness\" or centrality of the nodes\n",
    "            degree_u = graph.degree(u)\n",
    "            degree_v = graph.degree(v)\n",
    "            \n",
    "            topo_features = np.array([\n",
    "                cn, jaccard, pa, aa, ra, degree_u, degree_v,\n",
    "            ])\n",
    "            \n",
    "            # Combine all features\n",
    "            combined_features = np.concatenate([combined_features, topo_features])\n",
    "        \n",
    "        features_list.append(combined_features)\n",
    "    \n",
    "    return np.array(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 10496/10496 [00:00<00:00, 54370.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract features for training data\n",
    "train_features = extract_features(train_edges, train_graph, node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 10, 'class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "# Use Stratified k-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "if DO_HYPERPARAMETER_TUNING:\n",
    "    # Use Stratified k-fold cross-validation\n",
    "\n",
    "    # parameter grid for Random Forest\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 400],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    }\n",
    "\n",
    "    base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "    # Randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring=scorer,\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Find the best hyperparameters\n",
    "    random_search.fit(train_features, train_labels)\n",
    "\n",
    "    best_params = random_search.best_params_\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    \n",
    "    # Use the best hyperparameters to train the model\n",
    "    model = RandomForestClassifier(\n",
    "        **best_params,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True\n",
    "    )\n",
    "else:\n",
    "    # Use default hyperparameters\n",
    "    param_dist = {\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 15,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features': 'sqrt',\n",
    "        'class_weight': 'balanced'\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        **param_dist,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True\n",
    "    )\n",
    "    print(f\"Parameters: {param_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation AUC: 0.8356 ± 0.0088\n",
      "Each Fold AUC: [0.83999637 0.83799355 0.8192864  0.83543692 0.84545826]\n"
     ]
    }
   ],
   "source": [
    "# cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    model,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"Cross-Validation AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "print(f'Each Fold AUC: {cv_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Error Rate: 0.2448\n"
     ]
    }
   ],
   "source": [
    "# Use all training data to train the model\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# Check the OOB error rate\n",
    "oob_error = 1 - model.oob_score_\n",
    "print(f\"OOB Error Rate: {oob_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 3498/3498 [00:00<00:00, 35736.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract features for test data\n",
    "test_features = extract_features(test_edges, train_graph, node_features)\n",
    "\n",
    "test_probs = model.predict_proba(test_features)[:, 1]\n",
    "test_preds = (test_probs >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /Users/cck/Desktop/OneDrive_France/DSBA/T2/Machine Learning in Network Science/Kaggle/code/rf_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# set output file\n",
    "output_file = os.path.join(base_path, \"rf_predictions.csv\")\n",
    "\n",
    "# Output predictions\n",
    "with open(output_file, \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"ID\", \"Predicted\"])\n",
    "    for i, pred in enumerate(test_preds):\n",
    "        writer.writerow([i, int(pred)])\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
